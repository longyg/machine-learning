{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Vectorization\n",
    "This notebook introduces the ways to convert texts to vectors.\n",
    "\n",
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'there': 11, 'used': 13, 'to': 12, 'be': 1, 'stone': 10, 'age': 0, 'bronze': 2, 'iron': 4, 'was': 14, 'of': 8, 'revolution': 9, 'now': 7, 'it': 6, 'is': 5, 'digital': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\n",
    "    'There used to be Stone Age',\n",
    "    'There used to be Bronze Age bronze',\n",
    "    'There used to be Iron Age',\n",
    "    'There was Age of Revolution',\n",
    "    'Now it is Digital Age'\n",
    "]\n",
    "vectorizer = CountVectorizer(analyzer='word')\n",
    "vec = vectorizer.fit_transform(texts)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 1)\t1\n",
      "  (0, 12)\t1\n",
      "  (0, 13)\t1\n",
      "  (0, 11)\t1\n",
      "  (1, 2)\t2\n",
      "  (1, 0)\t1\n",
      "  (1, 1)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 11)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 12)\t1\n",
      "  (2, 13)\t1\n",
      "  (2, 11)\t1\n",
      "  (3, 9)\t1\n",
      "  (3, 8)\t1\n",
      "  (3, 14)\t1\n",
      "  (3, 0)\t1\n",
      "  (3, 11)\t1\n",
      "  (4, 3)\t1\n",
      "  (4, 5)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 7)\t1\n",
      "  (4, 0)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 0 0 0 0 0 0 0 0 1 1 1 1 0]\n",
      " [1 1 2 0 0 0 0 0 0 0 0 1 1 1 0]\n",
      " [1 1 0 0 1 0 0 0 0 0 0 1 1 1 0]\n",
      " [1 0 0 0 0 0 0 0 1 1 0 1 0 0 1]\n",
      " [1 0 0 1 0 1 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 0 0 0 0 0 0 0 0 0 1 1 0 0 1]]\n"
     ]
    }
   ],
   "source": [
    "vec = vectorizer.transform([ 'There was Stone Age'])\n",
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'there used': 13, 'used to': 16, 'to be': 15, 'be stone': 4, 'stone age': 12, 'be bronze': 2, 'bronze age': 5, 'age bronze': 0, 'be iron': 3, 'iron age': 7, 'there was': 14, 'was age': 17, 'age of': 1, 'of revolution': 11, 'now it': 10, 'it is': 9, 'is digital': 8, 'digital age': 6}\n"
     ]
    }
   ],
   "source": [
    "texts = [\n",
    "    'There used to be Stone Age',\n",
    "    'There used to be Bronze Age bronze',\n",
    "    'There used to be Iron Age',\n",
    "    'There was Age of Revolution',\n",
    "    'Now it is Digital Age'\n",
    "]\n",
    "vectorizer = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
    "vec = vectorizer.fit_transform(texts)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 15)\t1\n",
      "  (0, 16)\t1\n",
      "  (0, 13)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 2)\t1\n",
      "  (1, 15)\t1\n",
      "  (1, 16)\t1\n",
      "  (1, 13)\t1\n",
      "  (2, 7)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 15)\t1\n",
      "  (2, 16)\t1\n",
      "  (2, 13)\t1\n",
      "  (3, 11)\t1\n",
      "  (3, 1)\t1\n",
      "  (3, 17)\t1\n",
      "  (3, 14)\t1\n",
      "  (4, 6)\t1\n",
      "  (4, 8)\t1\n",
      "  (4, 9)\t1\n",
      "  (4, 10)\t1\n"
     ]
    }
   ],
   "source": [
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 1 0 0 0 0 0 0 0 1 1 0 1 1 0]\n",
      " [1 0 1 0 0 1 0 0 0 0 0 0 0 1 0 1 1 0]\n",
      " [0 0 0 1 0 0 0 1 0 0 0 0 0 1 0 1 1 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 1]\n",
      " [0 0 0 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "vec = vectorizer.transform([ 'There was Stone Age'])\n",
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'there': 11, 'used': 13, 'to': 12, 'be': 1, 'stone': 10, 'age': 0, 'bronze': 2, 'iron': 4, 'was': 14, 'of': 8, 'revolution': 9, 'now': 7, 'it': 6, 'is': 5, 'digital': 3}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "texts = [\n",
    "    'There used to be Stone Age',\n",
    "    'There used to be Bronze Age bronze',\n",
    "    'There used to be Iron Age',\n",
    "    'There was Age of Revolution',\n",
    "    'Now it is Digital Age'\n",
    "]\n",
    "vectorizer = TfidfVectorizer(analyzer='word', smooth_idf=True)\n",
    "vec = vectorizer.fit_transform(texts)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 11)\t0.33140159786840845\n",
      "  (0, 13)\t0.3939481437168047\n",
      "  (0, 12)\t0.3939481437168047\n",
      "  (0, 1)\t0.3939481437168047\n",
      "  (0, 10)\t0.5882354607969754\n",
      "  (0, 0)\t0.28029734885918384\n",
      "  (1, 11)\t0.23213777065833785\n",
      "  (1, 13)\t0.27594991824306836\n",
      "  (1, 12)\t0.27594991824306836\n",
      "  (1, 1)\t0.27594991824306836\n",
      "  (1, 0)\t0.1963406395869283\n",
      "  (1, 2)\t0.8240857580041683\n",
      "  (2, 11)\t0.33140159786840845\n",
      "  (2, 13)\t0.3939481437168047\n",
      "  (2, 12)\t0.3939481437168047\n",
      "  (2, 1)\t0.3939481437168047\n",
      "  (2, 0)\t0.28029734885918384\n",
      "  (2, 4)\t0.5882354607969754\n",
      "  (3, 11)\t0.2992461174212536\n",
      "  (3, 0)\t0.25310044945192844\n",
      "  (3, 14)\t0.5311597134872388\n",
      "  (3, 8)\t0.5311597134872388\n",
      "  (3, 9)\t0.5311597134872388\n",
      "  (4, 0)\t0.2317654623904255\n",
      "  (4, 7)\t0.48638584746139363\n",
      "  (4, 6)\t0.48638584746139363\n",
      "  (4, 5)\t0.48638584746139363\n",
      "  (4, 3)\t0.48638584746139363\n"
     ]
    }
   ],
   "source": [
    "print(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.28029735 0.39394814 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.58823546 0.3314016\n",
      "  0.39394814 0.39394814 0.        ]\n",
      " [0.19634064 0.27594992 0.82408576 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.23213777\n",
      "  0.27594992 0.27594992 0.        ]\n",
      " [0.28029735 0.39394814 0.         0.         0.58823546 0.\n",
      "  0.         0.         0.         0.         0.         0.3314016\n",
      "  0.39394814 0.39394814 0.        ]\n",
      " [0.25310045 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.53115971 0.53115971 0.         0.29924612\n",
      "  0.         0.         0.53115971]\n",
      " [0.23176546 0.         0.         0.48638585 0.         0.48638585\n",
      "  0.48638585 0.48638585 0.         0.         0.         0.\n",
      "  0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.29872406 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.62690599 0.3531879\n",
      "  0.         0.         0.62690599]]\n"
     ]
    }
   ],
   "source": [
    "vec = vectorizer.transform([ 'There was Stone Age'])\n",
    "print(vec.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gemsim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'age': 0, 'be': 1, 'stone': 2, 'there': 3, 'to': 4, 'used': 5, 'bronze': 6, 'iron': 7, 'of': 8, 'revolution': 9, 'was': 10, 'digital': 11, 'is': 12, 'it': 13, 'now': 14}\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "\n",
    "corpus = [\n",
    "    'There used to be Stone Age',\n",
    "    'There used to be Bronze Age bronze',\n",
    "    'There used to be Iron Age',\n",
    "    'There was Age of Revolution',\n",
    "    'Now it is Digital Age'\n",
    "]\n",
    "texts = [[word for word in document.lower().split()] for document in corpus]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1)], [(0, 1), (1, 1), (3, 1), (4, 1), (5, 1), (6, 2)], [(0, 1), (1, 1), (3, 1), (4, 1), (5, 1), (7, 1)], [(0, 1), (3, 1), (8, 1), (9, 1), (10, 1)], [(0, 1), (11, 1), (12, 1), (13, 1), (14, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(1, 0.27610534488362876), (2, 0.8699140943739595), (3, 0.12061087840494013), (4, 0.27610534488362876), (5, 0.27610534488362876)]\n",
      "[(1, 0.1526807310725103), (3, 0.06669540243027648), (4, 0.1526807310725103), (5, 0.1526807310725103), (6, 0.9620901758006651)]\n",
      "[(1, 0.27610534488362876), (3, 0.12061087840494013), (4, 0.27610534488362876), (5, 0.27610534488362876), (7, 0.8699140943739595)]\n",
      "[(3, 0.07979258234193365), (8, 0.5755093812740171), (9, 0.5755093812740171), (10, 0.5755093812740171)]\n",
      "[(11, 0.5), (12, 0.5), (13, 0.5), (14, 0.5)]\n"
     ]
    }
   ],
   "source": [
    "tfidf = models.TfidfModel(corpus)\n",
    "corpus_tfidf = tfidf[corpus]\n",
    "for doc in corpus_tfidf:\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (2, 1), (3, 1), (10, 1)]\n",
      "[(2, 0.7037329116471254), (3, 0.09757037526482304), (10, 0.7037329116471254)]\n"
     ]
    }
   ],
   "source": [
    "query = 'There was Stone Age'\n",
    "query_bow = dictionary.doc2bow(query.lower().split())\n",
    "print(query_bow)\n",
    "query_tfidf = tfidf[query_bow]\n",
    "print(query_tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<keras_preprocessing.text.Tokenizer object at 0x7fba046334a8>\n",
      "[[2, 3, 4, 5, 7, 1], [2, 3, 4, 5, 6, 1, 6], [2, 3, 4, 5, 8, 1], [2, 9, 1, 10, 11], [12, 13, 14, 15, 1]]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing import text\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "\n",
    "train_texts = [\n",
    "    'There used to be Stone Age',\n",
    "    'There used to be Bronze Age bronze',\n",
    "    'There used to be Iron Age',\n",
    "    'There was Age of Revolution',\n",
    "    'Now it is Digital Age'\n",
    "]\n",
    "\n",
    "tokenizer = text.Tokenizer(num_words=20000)\n",
    "tokenizer.fit_on_texts(train_texts)\n",
    "print(tokenizer)\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(train_texts)\n",
    "print(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "MAX_SEQUENCE_LENGTH = 500\n",
    "max_len = len(max(x_train, key=len))\n",
    "if max_len > MAX_SEQUENCE_LENGTH:\n",
    "    max_len = MAX_SEQUENCE_LENGTH\n",
    "print(max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2  3  4  5  7  1  0]\n",
      " [ 2  3  4  5  6  1  6]\n",
      " [ 2  3  4  5  8  1  0]\n",
      " [ 2  9  1 10 11  0  0]\n",
      " [12 13 14 15  1  0  0]]\n"
     ]
    }
   ],
   "source": [
    "pad_x_train = sequence.pad_sequences(x_train, maxlen=7, padding='post')\n",
    "print(pad_x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2, 9, 7, 1]]\n",
      "[[2 9 7 1 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "seq = tokenizer.texts_to_sequences(['There was Stone Age'])\n",
    "print(seq)\n",
    "pad_seq = sequence.pad_sequences(seq, maxlen=7, padding='post')\n",
    "print(pad_seq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 1, 4]\n"
     ]
    }
   ],
   "source": [
    "one_hot = text.one_hot('my name is Join', n=12)\n",
    "print(one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
